{"pageProps":{"posts":[{"data":{"title":"Go RPCs (Remote Procedure Calls)","date":"2021-05-28T11:04:32+05:30","draft":false,"tags":["golang"]},"content":"\n# Go RPCs (Remote Procedure Calls)\n\n## Introduction\n\nGo's `rpc` package enables communication across multiple machines, sort of like inter-process communication via funtion/method/procedure calls.\n\nAs the Go documentation puts it:\n> Package `rpc` provides access to the exported methods of an object across a network or other I/O connection. A server registers an object, making it visible as a service with the name of the type of the object.\n\nThe object that is registered with the `rpc` server can be either primitive/builtin data types (`int`, `string` etc.) or user-defined ones (such as `struct`s). I would usually prefer a `struct`.\n\nUnder the hood, by default, Go uses `gob` for serializing and deserializing the remote procedure calls. This is configurable, that is, you can plug-in `JSON` or other custom codecs.\n\n## Prerequisites\n\nThe methods which define actions on the registered object must adhere to a specific function signature. The methods,\n- and its type(s) must be exported (uppercased)\n- have two arguments, both exported (or builtin) types.\n- their second argument is a pointer.\n- must have one return type, `error`.\n\nEssentially the method signature looks like the following:\n```go\nfunc (t *T) MethodName(argType T1, replyType *T2) error\n```\n\n## Implementation\n\nLet's look at a dummy blog server and a client interacting with each other via `rpc`.\n### a. Types\n\nIt's a good idea to wrap the object, along with the methods and types, into a package of its own. This makes it easier for both the server and client implementations to agree on common type definitions.\n\nIn the following code, `Blog` will be registered with the `rpc` server.\n```go\ntype Blog struct {\n\tposts      map[int]Post\n\tlastPostID int\n\tsync.Mutex\n}\n\ntype Post struct {\n\tID    int\n\tTitle string\n\tBody  string\n}\n\n// NewBlog is required because, even though Blog is exported its fields are not.\n// The fields are internal to the package.\nfunc NewBlog() *Blog {\n\treturn &Blog{\n\t\tposts: make(map[int]Post),\n\t}\n}\n```\nThe function `NewBlog() *Blog` is necessary since we aren't exporting its fields: `posts` and `lastPostID`.\n\n### b. Methods\n`Blog` will have two methods: `AddPost` and `GetPostByID`. Notice the function signature.\n```go\nfunc (b *Blog) AddPost(payload, reply *Post) error {\n\tb.Lock()\n\tdefer b.Unlock()\n\n\tif payload.Title == \"\" || payload.Body == \"\" {\n\t\treturn errors.New(\"Title and Body must not be empty\")\n\t}\n\n\tb.lastPostID++\n\n\t*reply = Post{ID: b.lastPostID, Title: payload.Title, Body: payload.Body}\n\tb.posts[reply.ID] = *reply\n\n\treturn nil\n}\n\nfunc (b *Blog) GetPostByID(payload int, reply *Post) error {\n\tb.Lock()\n\tdefer b.Unlock()\n\n\t*reply = b.posts[payload]\n\n\treturn nil\n}\n```\n\nEach method takes two arguments, `payload` and `reply` (you can name them anything). It receives the input from the client in the first parameter. The second argument (a pointer) is used for sending a response back to the client.\n\n### c. Server\nThe server implementation is fairly simple:\n```go\n// Fetch an instance of the object\nblog := types.NewBlog()\n\nrpc.Register(blog) // Register the instance with the rpc\nrpc.HandleHTTP() // Configure the rpc to serve over HTTP\n\nerr := http.ListenAndServe(\":3000\", nil)\nif err != nil {\n\tlog.Fatalln(\"Error starting the RPC server\", err)\n}\n```\n\n### d. Client\n\nDial the server using `rpc.DialHTTP`:\n```go\nclient, err := rpc.DialHTTP(\"tcp\", \":3000\")\nif err != nil {\n\tlog.Fatalln(\"Error creating the RPC client\", err)\n}\n```\n\nAnd make the remote procedure calls using `client.Call`. It takes three arguments:\n1. The method name of the form: `<T>.<MethodName>`\n2. Input params\n3. A pointer to receive the response from the server\n```go\n// Create a post\nvar post types.Post\n\n// Create posts\nerr = client.Call(\"Blog.AddPost\", &types.Post{Title: \"post 1\", Body: \"Hello, world!\"}, &post)\nif err != nil {\n\tlog.Fatalln(\"Error creating post\", err)\n}\nlog.Printf(\"[AddPost] ID: %d | Title: %s | Body: %s\\n\", post.ID, post.Title, post.Body)\n\n// Fetch a post by ID\nerr = client.Call(\"Blog.GetPostByID\", 3, &post)\nif err != nil {\n\tlog.Fatalln(\"Error creating post\", err)\n}\nlog.Printf(\"[GetPostByID] ID: %d | Title: %s | Body: %s\\n\", post.ID, post.Title, post.Body)\n```\n\nThe entire code can be found on [Github](https://github.com/bensooraj/blog-code-resource-repo/tree/main/borneo).\n\n## Some thoughts\n\nIn our implementation, the initial handshake between the `rpc` client and server are negotiated over HTTP. Thereafter, the HTTP server acts like a proxy or a conduit between the two, a concept known as [HTTP tunneling](https://en.wikipedia.org/wiki/HTTP_tunnel). I believe this can be customized.\n\nIt can be difficult to standardize (to follow common semantics) RPC implementations. This gap is what projects like `gRPC`, `dRPC` etc. attepmt to solve.\n\nIt should be obvious by now that you can register only one object with a given name with the `rpc` server.\n\nFurther reading:\n1. [godoc: Package `rpc`](https://golang.org/pkg/net/rpc/)\n2. [gRPC: A high performance, open source universal RPC framework](https://grpc.io/)\n3. [dRPC: A lightweight, drop-in replacement for gRPC](https://github.com/storj/drpc)\n\n---\n\nNote: _This article is not an in-depth tutorial or treatment of Golang's syntax, semantics, design or implementation, but a journal of my learnings._","slug":"6-go-rpc"},{"data":{"title":"The empty interface and the empty struct in golang","date":"2020-09-01T17:04:32+05:30","draft":false,"tags":["golang"]},"content":"\n# The empty interface and the empty struct in golang\n\nAn `interface` is a collection or set of method declarations. A data type implements or satisfies an `interface` if it at least defines the methods declared by the `interface`.\n\n## Empty interface\n\nAn empty interface `interface{}` has zero methods. So, in essence, any data type implements or satisfies an empty interface. Let's take the following example:\n\n```go\ntype Container []interface{}\n\nfunc (c *Container) Put(elem interface{}) {\n    *c = append(*c, elem)\n    switch elem.(type) {\n    case int:\n        fmt.Println(\"Put \", elem, \" of type int\")\n    case string:\n        fmt.Println(\"Put \", elem, \" of type string\")\n    default:\n        fmt.Println(\"Put \", elem, \" of type unknown\")\n    }\n}\n\nfunc (c *Container) Pop() interface{} {\n    elem := (*c)[0]\n    *c = (*c)[1:]\n    return elem\n}\n\nfunc main() {\n    allContainer := &Container{}\n\n    allContainer.Put(\"Hello\")\n    allContainer.Put(213)\n    allContainer.Put(123.321)\n\n    fmt.Printf(\"allContainer: %+v\\n\\n\", allContainer)\n}\n```\n\nOutput:\n\n```sh\n$ go run main.go\nPut  Hello  of type string\nPut  213  of type int\nPut  123.321  of type unknown\nallContainer: &[Hello 213 123.321]\n```\n\nIn the above example, the data type `Container` is a slice of empty interfaces with two methods defined on it, `Put()` and `Pop()`. It loosely resembles a _First In, Last Out_ stack.\n\nThe `Put()` method takes an `interface{}` as the input argument and appends it to the slice of `interface{}`s. This means that it can accept a string, an integer, a float or any other simple and composite data type as its input argument. You can access the underlying type of `elem`, an `interface{}`, using `elem.(type)`.\n\nThis approach does have its disadvantages though. One being that you should implement type assertions, checks or type specific logics to avoid any surprises during runtime.\n\n## Empty struct\n\nTo quote the Golang spec:\n\n> A struct is a sequence of named elements, called fields, each of which has a name and a type.\n\nLet's take an example\n\n```go\n// A struct with 5 fields.\nstruct {\n    x, y int\n    u float32\n    A *[]int\n    F func()\n}\n```\n\nAn empty struct is a struct data type with zero fields:\n\n```go\n// Named type\ntype EmptyStruct struct{}\n\n// Variable declaration\nvar es struct{}\n\n// Or use it directly!\nstruct{}{}\n```\n\nThe size or width of a `struct` is defined as the sum of its constituent types. An empty `struct`, since it has no fields within it, has a size or width of zero. Zero bytes!\n\nJust like regular structs you can define methods on empty structs as well. Sort of like a zero sized container for methods:\n\n```go\ntype EmptyStruct struct{}\n\nfunc (es *EmptyStruct) WhoAmI() {\n    fmt.Println(\"I am an empty struct!\")\n}\n```\n\nEmpty structs find its best use case in channel signalling. Many a mere mortals such as me have used booleans or integers to notify an event over a channel.\n\nThe following example prints the current time at every 500 milliseconds and times out after 3 seconds,\n\n```go\nfunc printFor3Seconds(doneChannel chan struct{}) {\n    ticker := time.NewTicker(500 * time.Millisecond)\n    timeout := time.After(3 * time.Second)\n\n    for {\n        select {\n        case t := <-ticker.C:\n            fmt.Printf(\"Tik tik: %v\\n\", t)\n        case <-timeout:\n            fmt.Printf(\"Timeout at: %v\\n\", time.Now())\n            doneChannel <- struct{}{}\n            return\n        }\n    }\n}\n\nfunc main() {\n    doneChannel := make(chan struct{}, 1)\n\n    go printFor3Seconds(doneChannel)\n\n    <-doneChannel\n}\n\n// Output\n// $ go run 1.go\n// Tik tik: 2020-09-16 08:25:36.750631 +0530 IST m=+0.501680080\n// Tik tik: 2020-09-16 08:25:37.250303 +0530 IST m=+1.001337269\n// Tik tik: 2020-09-16 08:25:37.750696 +0530 IST m=+1.501714888\n// Tik tik: 2020-09-16 08:25:38.253708 +0530 IST m=+2.004711715\n// Tik tik: 2020-09-16 08:25:38.752804 +0530 IST m=+2.503793180\n// Tik tik: 2020-09-16 08:25:39.251941 +0530 IST m=+3.002914584\n// Timeout at: 2020-09-16 08:25:39.251999 +0530 IST m=+3.002972846\n```\n\nInstead of `doneChannel <- 0` or `doneChannel <- true`, I am using `doneChannel <- struct{}{}` for channel signalling. Using booleans or integers for channel signalling involves memory allocation, copying over the element etc.\n\nWe can use empty structs for simulating sets as well,\n\n```go\nintSet := make(map[int]struct{})\nempty := struct{}{}\n\n// We are setting empty as the value for 1, because the\n// value is meaningless\nintSet[1] = empty\nif _, ok := intSet[1]; ok {\n    fmt.Println(\"1 is in the set\")\n}\n\n// 2 is not one of intSet's keys\nif _, ok := intSet[2]; !ok {\n    fmt.Println(\"2 is not in the set\")\n}\n```\n\n## Resources:\n\n1. [The Go Programming Language Specification](https://golang.org/ref/spec#Struct_types)\n2. [The empty struct - Dave Cheney](https://dave.cheney.net/2014/03/25/the-empty-struct)\n\n---\n\nNote: _This article is not an in-depth tutorial or treatment of Golang's syntax, semantics, design or implementation, but a journal of my learnings._","slug":"4-empty-interface-and-struct"},{"data":{"title":"Sending application logs from EKS to Datadog","date":"2020-08-10T17:04:32+05:30","draft":false,"tags":["architecture"]},"content":"\nimport MDImage from '../../../components/MDImage.js';\n\n# Sending application logs from EKS to Datadog\n\nDatadog has been part of our stack for quite sometime now, where the `datadog-agent` installed on the EC2 instances (baked into the image) tail log files to which the application services write to.\n\nA recent project required me to send the application logs deployed on kubernetes to Datadog. This required a different approach which I would be sharing today. A few pre-requisites before I begin:\n\n1. An up and running kubernetes cluster. I use [`eksctl`](https://eksctl.io/) to quickly spin up a k8s cluster on AWS EKS.\n2. A [Datadog](https://www.datadoghq.com/) account\n3. Helm v3 installed (v3 doesn't require you to setup Tiller (Helm's server component) on your kubernetes cluster)\n\n## Pre-requisites\n\nTo set the context, I have a 3-node cluster set up:\n```sh\n$ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE   VERSION\nip-10-0-0-34.ap-south-1.compute.internal    Ready    <none>   9d    v1.16.13-eks-2ba888\nip-10-0-1-144.ap-south-1.compute.internal   Ready    <none>   9d    v1.16.13-eks-2ba888\nip-10-0-2-79.ap-south-1.compute.internal    Ready    <none>   9d    v1.16.13-eks-2ba888\n``` \n\nAdd and update the Datadog chart repository,\n```sh\n# Ensure that you are running helm v3\n$ helm version --short\nv3.3.1+g249e521\n\n# Add and update the Datadog and stable chart repositories\n$ helm repo add datadog https://helm.datadoghq.com\n\"datadog\" has been added to your repositories\n\n$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/\n\"stable\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"stable\" chart repository\nUpdate Complete. ⎈Happy Helming!⎈\n\n$ helm repo list\nNAME   \tURL                                              \nstable \thttps://kubernetes-charts.storage.googleapis.com/\ndatadog\thttps://helm.datadoghq.com        \n\n# Search for datadog chart. We will be using datadog/datadog\n$ helm search repo datadog\nNAME           \tCHART VERSION\tAPP VERSION\tDESCRIPTION             \ndatadog/datadog\t2.4.13       \t7          \tDatadog Agent           \nstable/datadog \t2.3.42       \t7          \tDEPRECATED Datadog Agent\n```\n\nClone the git repository `bensooraj/node-eks-datadog-integration`:\n```sh\n$ git clone https://github.com/bensooraj/node-eks-datadog-integration.git\n\n# A quick overview overview of the project structure\n$ tree -L 2\n.\n├── Makefile\n├── README.md\n├── coverage\n├── datadog\n│   └── values.yml\n├── docker\n│   └── Dockerfile\n├── kubernetes\n│   ├── deployment.yaml\n│   └── service.yaml\n├── node_modules\n│   ├── @dabh\n│   ....\n│   ...\n│   ..\n│   ├── winston\n│   └── winston-transport\n├── package-lock.json\n├── package.json\n└── server.js\n\n83 directories, 9 files\n```\n\nCheck [datadog-values.yaml](https://github.com/DataDog/helm-charts/blob/master/charts/datadog/values.yaml) for the latest YAML file.\n\n## Deploy the application\n\nWe will be deploying an ultra simple Node.js API which logs out JSON formatted log messages whenever it receives a `GET` request:\n```js\n// server.js\nconst express = require('express');\nconst winston = require('winston');\n\n// Constants\nconst PORT = 8080;\nconst HOST = '0.0.0.0';\n\nconst app = express();\n\nconst logger = winston.createLogger({\n    transports: [\n        new winston.transports.Console({\n            handleExceptions: true,\n        })\n    ],\n    level: 'debug',\n    exitOnError: false,\n    format: winston.format.combine(\n        winston.format.splat(),\n        winston.format.timestamp(),\n        winston.format.json()\n    ),\n});\n\napp.get('/', (req, res) => {\n    logger.silly(\"This is a silly message\", { url: req.url, environment: \"test\" });\n    logger.debug(\"This is a debug message\", { url: req.url, environment: \"test\" });\n    logger.info(\"This is an info message\", { url: req.url, environment: \"test\" });\n\n    res.json({\n        response_message: 'Hello World'\n    });\n});\n\napp.listen(PORT, HOST);\nlogger.info(\"Running on http://${HOST}:${PORT}\", { host: HOST, port: PORT, environment: \"test\" });\n```\n\nRelevant sections from the deployment YAML file,\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eks-datadog-demo-node-app\n  annotations:\n    ad.datadoghq.com/eks-datadog-demo-node-app.logs: '[{\"source\":\"nodejs\",\"service\":\"eks-datadog-demo-node-app\"}]'  \n  labels:\n    tags.datadoghq.com/env: \"test\"\n    tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n    tags.datadoghq.com/version: \"v1.0.1\"\nspec:\n  # # #\n  # #\n  #\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/eks-datadog-demo-node-app.logs: '[{\"source\":\"nodejs\",\"service\":\"eks-datadog-demo-node-app\"}]'\n      labels:\n        app: eks-datadog-demo-node-app\n        tags.datadoghq.com/env: \"test\"\n        tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n\t\ttags.datadoghq.com/version: \"v1.0.1\"\n\tspec:\n      containers:\n        - name: eks-datadog-demo-node-app\n  # # #\n  # #\n  #\n```\nPay close attention to the `labels` and `annotations`. These are especially useful if you are planning to collect `traces` and application `metrics`, which are outside the scope of this article.\n\nIt's worth mentioning that you can exclude or include logs from deployments and/or pods by configuring the settings in the [`annotations`](https://docs.datadoghq.com/agent/logs/advanced_log_collection/?tab=kubernetes#exclude-at-match). For example, to exclude all logs from the above deployment configure the `log_processing_rules` in the `annotations` section as follows:\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eks-datadog-demo-node-app\n  annotations:\n    ad.datadoghq.com/eks-datadog-demo-node-app.logs: >-\n      [{\n        \"source\": \"nodejs\",\n        \"service\": \"eks-datadog-demo-node-app\",\n        \"log_processing_rules\": [{\n          \"type\": \"exclude_at_match\",\n          \"name\": \"exclude_this_deployment\",\n          \"pattern\":\"\\.*\"\n        }]\n      }]  \n  labels:\n    tags.datadoghq.com/env: \"test\"\n    tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n    tags.datadoghq.com/version: \"v1.0.1\"\nspec:\n  # # #\n  # #\n  #\n```\n\nDeploy the API and exopse it as a service,\n```sh\n# Deployment\n$ kubectl apply -f kubernetes/deployment.yaml\n\n# Service\n$ kubectl apply -f kubernetes/service.yaml\n\n# Verify that the API is up and running\n$ kubectl get deployment,svc,po\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/eks-datadog-demo-node-app          2/2     2            2           30h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/eks-datadog-demo-node-app          ClusterIP   172.20.205.150   <none>        8080/TCP   30h\n\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/eks-datadog-demo-node-app-7bf7cf764f-gpqvf          1/1     Running   0          29h\npod/eks-datadog-demo-node-app-7bf7cf764f-spb7m          1/1     Running   0          29h\n```\n\n## Deploy the Datadog agent\n\nOpen the file `datadog/values.yml` and ensure that `datadog.logs.enabled` and `datadog.logs.containerCollectAll` are set to `true`.\n```yaml\ndatadog:\n  ## @param logs - object - required\n  ## Enable logs agent and provide custom configs\n  #\n  logs:\n    ## @param enabled - boolean - optional - default: false\n    ## Enables this to activate Datadog Agent log collection.\n    #\n    enabled: true\n    ## @param containerCollectAll - boolean - optional - default: false\n    ## Enable this to allow log collection for all containers.\n    #\n    containerCollectAll: true\n```\n\nDeploy the datadog-agent:\n```sh\n$ helm install datadog-agent -f datadog/values.yml  --set datadog.apiKey=<DATADOG_API_KEY> datadog/datadog\n\n# Helm will deploy one datadog-agent pod per node\n$ kubectl get deployment,svc,po\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/datadog-agent-kube-state-metrics   1/1     1            1           29h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/datadog-agent-kube-state-metrics   ClusterIP   172.20.129.97    <none>        8080/TCP   29h\n\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/datadog-agent-b4mtv                                 2/2     Running   0          29h\npod/datadog-agent-f6jn7                                 2/2     Running   0          29h\npod/datadog-agent-zvp6p                                 2/2     Running   0          29h\n```\n\nNote that even though the agent is deployed in the `default` namespace, it can collect logs and metrics from deployments/pods from all namespaces.\n\n## O Logs! Where art thou?\n\nNow we have the API service (for generating logs) and the Datadog agent (for streaming the logs to Datadog) all set! I assume this is what happens behind the scenes:\n\n1. The API service writes logs to **`stdout`** and **`stderr`**\n2. The **`kubelet`** writes these logs to **`/var/log/pods/<namespace>_<pod_name>_<pod_id>/<container_name>/<num>.log`** on the node machine\n3. The **`datadog-agent`**s running on each node tails these log files and streams them to the Datadog servers\n\nAnyways, let's generate some logs:\n```sh\n# Port forward to access the API service from your local machine\n$ kubectl port-forward service/eks-datadog-demo-node-app 8080:8080\nForwarding from 127.0.0.1:8080 -> 8080\nForwarding from [::1]:8080 -> 8080\n\n# Generate some logs using curl or ab\n$ ab -n 50 -c 2 -l http://localhost:8080/\n\n# Verify that the logs are generated\nj$ kubectl logs -l app=eks-datadog-demo-node-app --since=100m\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"debug\",\"message\":\"This is a debug message\",\"timestamp\":\"2020-09-04T19:35:52.126Z\"}\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"info\",\"message\":\"This is an info message\",\"timestamp\":\"2020-09-04T19:35:53.308Z\"}\n....\n...\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"debug\",\"message\":\"This is a debug message\",\"timestamp\":\"2020-09-04T19:35:53.310Z\"}\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"info\",\"message\":\"This is an info message\",\"timestamp\":\"2020-09-04T19:35:53.310Z\"}\n```\n\nNavigate to `Log Explorer` in Datadog to see the logs that we generated above\n\n<MDImage src=\"/5/2_dd_ui_overview.png\" alt=\"Datadog UI overview\" />\n\nClick on any one of the log records to view more details\n\n<MDImage src=\"/5/2_dd_log_detail_view.png\" alt=\"Datadog log record detail view\" />\n\nOn the left pane, you can filter the logs by namespace, pods etc. as well!\n\n<MDImage src=\"/5/2_filter_ui_ns.png\" alt=\"Datadog log record detail view UI\" />\n\nThat's it for now!\n\nI hope this helped and if you have any feedback for me, please let me know :smile:.\n\nFurther reading:\n1. [Logging Architecture](https://kubernetes.io/docs/concepts/cluster-administration/logging/)\n2. [Datadog kubernetes log collection](https://docs.datadoghq.com/agent/kubernetes/log/?tab=daemonset#log-collection)\n3. [Datadog agent as a DaemonSet (recommended)](https://docs.datadoghq.com/agent/kubernetes/?tab=daemonset)\n---\n\nNote: _This article is not an in-depth tutorial on implemending logging for applications running on kubernetes, but a journal of my learnings._","slug":"5-eks-datadog"}]},"__N_SSG":true}