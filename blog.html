<!DOCTYPE html><html lang="en"><head><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/github-dark.min.css"/><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="./_next/static/css/cd63f2b0468b5f7a.css" as="style"/><link rel="stylesheet" href="./_next/static/css/cd63f2b0468b5f7a.css" data-n-g=""/><link rel="preload" href="./_next/static/css/d0b847fa47fa8a5b.css" as="style"/><link rel="stylesheet" href="./_next/static/css/d0b847fa47fa8a5b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="./_next/static/chunks/webpack-ed47cc4bbff8e103.js" defer=""></script><script src="./_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="./_next/static/chunks/main-7e73d61bce33e887.js" defer=""></script><script src="./_next/static/chunks/pages/_app-e6ebfa373a417a20.js" defer=""></script><script src="./_next/static/chunks/pages/blog-7942542a44d812be.js" defer=""></script><script src="./_next/static/vZF5O8Cp0wJbkUP_UfCYA/_buildManifest.js" defer=""></script><script src="./_next/static/vZF5O8Cp0wJbkUP_UfCYA/_ssgManifest.js" defer=""></script><script src="./_next/static/vZF5O8Cp0wJbkUP_UfCYA/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div><header class="Header_header__VYZ3G"><div class="max-width-container"><h2><a href="/">Ben Sooraj</a></h2><ul><li><a href="/about">About</a></li><li><a href="/blog">Blog</a></li></ul></div></header><main class="max-width-container main"><h1>Blog</h1><hr/><div class="Home_articleList__RW0kf"><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">28.04.2021</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/6-go-rpc">Go RPCs (Remote Procedure Calls)</a></h3><br style="clear:both"/></div><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">01.08.2020</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/4-empty-interface-and-struct">The empty interface and the empty struct in golang</a></h3><br style="clear:both"/></div><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">10.07.2020</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/5-eks-datadog">Sending application logs from EKS to Datadog</a></h3><br style="clear:both"/></div><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">02.07.2020</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/3-go-struct-aggregation-and-embedding">Struct aggregation and embedding in golang</a></h3><br style="clear:both"/></div><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">09.09.2019</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/2-up-and-running-with-kafka-on-aws-eks-using-strimzi">Up And Running With Kafka On AWS EKS Using Strimzi</a></h3><br style="clear:both"/></div><div class="Home_postItem__bXXHD"><h3 style="text-align:left;float:left;padding-right:1em;margin-top:10px;margin-bottom:5px">03.09.2019</h3><h3 style="text-align:left;float:left;margin-top:10px;margin-bottom:5px"><a href="/blog/0-accessing-amazon-rds-from-aws-eks">Accessing Amazon RDS From AWS EKS</a></h3><br style="clear:both"/></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"data":{"title":"Go RPCs (Remote Procedure Calls)","date":"2021-05-28T11:04:32+05:30","draft":false,"tags":["golang"]},"content":"\n# Go RPCs (Remote Procedure Calls)\n\n## Introduction\n\nGo's `rpc` package enables communication across multiple machines, sort of like inter-process communication via funtion/method/procedure calls.\n\nAs the Go documentation puts it:\n\u003e Package `rpc` provides access to the exported methods of an object across a network or other I/O connection. A server registers an object, making it visible as a service with the name of the type of the object.\n\nThe object that is registered with the `rpc` server can be either primitive/builtin data types (`int`, `string` etc.) or user-defined ones (such as `struct`s). I would usually prefer a `struct`.\n\nUnder the hood, by default, Go uses `gob` for serializing and deserializing the remote procedure calls. This is configurable, that is, you can plug-in `JSON` or other custom codecs.\n\n## Prerequisites\n\nThe methods which define actions on the registered object must adhere to a specific function signature. The methods,\n- and its type(s) must be exported (uppercased)\n- have two arguments, both exported (or builtin) types.\n- their second argument is a pointer.\n- must have one return type, `error`.\n\nEssentially the method signature looks like the following:\n```go\nfunc (t *T) MethodName(argType T1, replyType *T2) error\n```\n\n## Implementation\n\nLet's look at a dummy blog server and a client interacting with each other via `rpc`.\n### a. Types\n\nIt's a good idea to wrap the object, along with the methods and types, into a package of its own. This makes it easier for both the server and client implementations to agree on common type definitions.\n\nIn the following code, `Blog` will be registered with the `rpc` server.\n```go\ntype Blog struct {\n\tposts      map[int]Post\n\tlastPostID int\n\tsync.Mutex\n}\n\ntype Post struct {\n\tID    int\n\tTitle string\n\tBody  string\n}\n\n// NewBlog is required because, even though Blog is exported its fields are not.\n// The fields are internal to the package.\nfunc NewBlog() *Blog {\n\treturn \u0026Blog{\n\t\tposts: make(map[int]Post),\n\t}\n}\n```\nThe function `NewBlog() *Blog` is necessary since we aren't exporting its fields: `posts` and `lastPostID`.\n\n### b. Methods\n`Blog` will have two methods: `AddPost` and `GetPostByID`. Notice the function signature.\n```go\nfunc (b *Blog) AddPost(payload, reply *Post) error {\n\tb.Lock()\n\tdefer b.Unlock()\n\n\tif payload.Title == \"\" || payload.Body == \"\" {\n\t\treturn errors.New(\"Title and Body must not be empty\")\n\t}\n\n\tb.lastPostID++\n\n\t*reply = Post{ID: b.lastPostID, Title: payload.Title, Body: payload.Body}\n\tb.posts[reply.ID] = *reply\n\n\treturn nil\n}\n\nfunc (b *Blog) GetPostByID(payload int, reply *Post) error {\n\tb.Lock()\n\tdefer b.Unlock()\n\n\t*reply = b.posts[payload]\n\n\treturn nil\n}\n```\n\nEach method takes two arguments, `payload` and `reply` (you can name them anything). It receives the input from the client in the first parameter. The second argument (a pointer) is used for sending a response back to the client.\n\n### c. Server\nThe server implementation is fairly simple:\n```go\n// Fetch an instance of the object\nblog := types.NewBlog()\n\nrpc.Register(blog) // Register the instance with the rpc\nrpc.HandleHTTP() // Configure the rpc to serve over HTTP\n\nerr := http.ListenAndServe(\":3000\", nil)\nif err != nil {\n\tlog.Fatalln(\"Error starting the RPC server\", err)\n}\n```\n\n### d. Client\n\nDial the server using `rpc.DialHTTP`:\n```go\nclient, err := rpc.DialHTTP(\"tcp\", \":3000\")\nif err != nil {\n\tlog.Fatalln(\"Error creating the RPC client\", err)\n}\n```\n\nAnd make the remote procedure calls using `client.Call`. It takes three arguments:\n1. The method name of the form: `\u003cT\u003e.\u003cMethodName\u003e`\n2. Input params\n3. A pointer to receive the response from the server\n```go\n// Create a post\nvar post types.Post\n\n// Create posts\nerr = client.Call(\"Blog.AddPost\", \u0026types.Post{Title: \"post 1\", Body: \"Hello, world!\"}, \u0026post)\nif err != nil {\n\tlog.Fatalln(\"Error creating post\", err)\n}\nlog.Printf(\"[AddPost] ID: %d | Title: %s | Body: %s\\n\", post.ID, post.Title, post.Body)\n\n// Fetch a post by ID\nerr = client.Call(\"Blog.GetPostByID\", 3, \u0026post)\nif err != nil {\n\tlog.Fatalln(\"Error creating post\", err)\n}\nlog.Printf(\"[GetPostByID] ID: %d | Title: %s | Body: %s\\n\", post.ID, post.Title, post.Body)\n```\n\nThe entire code can be found on [Github](https://github.com/bensooraj/blog-code-resource-repo/tree/main/borneo).\n\n## Some thoughts\n\nIn our implementation, the initial handshake between the `rpc` client and server are negotiated over HTTP. Thereafter, the HTTP server acts like a proxy or a conduit between the two, a concept known as [HTTP tunneling](https://en.wikipedia.org/wiki/HTTP_tunnel). I believe this can be customized.\n\nIt can be difficult to standardize (to follow common semantics) RPC implementations. This gap is what projects like `gRPC`, `dRPC` etc. attepmt to solve.\n\nIt should be obvious by now that you can register only one object with a given name with the `rpc` server.\n\nFurther reading:\n1. [godoc: Package `rpc`](https://golang.org/pkg/net/rpc/)\n2. [gRPC: A high performance, open source universal RPC framework](https://grpc.io/)\n3. [dRPC: A lightweight, drop-in replacement for gRPC](https://github.com/storj/drpc)\n\n---\n\nNote: _This article is not an in-depth tutorial or treatment of Golang's syntax, semantics, design or implementation, but a journal of my learnings._","slug":"6-go-rpc"},{"data":{"title":"The empty interface and the empty struct in golang","date":"2020-09-01T17:04:32+05:30","draft":false,"tags":["golang"]},"content":"\n# The empty interface and the empty struct in golang\n\nAn `interface` is a collection or set of method declarations. A data type implements or satisfies an `interface` if it at least defines the methods declared by the `interface`.\n\n## Empty interface\n\nAn empty interface `interface{}` has zero methods. So, in essence, any data type implements or satisfies an empty interface. Let's take the following example:\n\n```go\ntype Container []interface{}\n\nfunc (c *Container) Put(elem interface{}) {\n    *c = append(*c, elem)\n    switch elem.(type) {\n    case int:\n        fmt.Println(\"Put \", elem, \" of type int\")\n    case string:\n        fmt.Println(\"Put \", elem, \" of type string\")\n    default:\n        fmt.Println(\"Put \", elem, \" of type unknown\")\n    }\n}\n\nfunc (c *Container) Pop() interface{} {\n    elem := (*c)[0]\n    *c = (*c)[1:]\n    return elem\n}\n\nfunc main() {\n    allContainer := \u0026Container{}\n\n    allContainer.Put(\"Hello\")\n    allContainer.Put(213)\n    allContainer.Put(123.321)\n\n    fmt.Printf(\"allContainer: %+v\\n\\n\", allContainer)\n}\n```\n\nOutput:\n\n```sh\n$ go run main.go\nPut  Hello  of type string\nPut  213  of type int\nPut  123.321  of type unknown\nallContainer: \u0026[Hello 213 123.321]\n```\n\nIn the above example, the data type `Container` is a slice of empty interfaces with two methods defined on it, `Put()` and `Pop()`. It loosely resembles a _First In, Last Out_ stack.\n\nThe `Put()` method takes an `interface{}` as the input argument and appends it to the slice of `interface{}`s. This means that it can accept a string, an integer, a float or any other simple and composite data type as its input argument. You can access the underlying type of `elem`, an `interface{}`, using `elem.(type)`.\n\nThis approach does have its disadvantages though. One being that you should implement type assertions, checks or type specific logics to avoid any surprises during runtime.\n\n## Empty struct\n\nTo quote the Golang spec:\n\n\u003e A struct is a sequence of named elements, called fields, each of which has a name and a type.\n\nLet's take an example\n\n```go\n// A struct with 5 fields.\nstruct {\n    x, y int\n    u float32\n    A *[]int\n    F func()\n}\n```\n\nAn empty struct is a struct data type with zero fields:\n\n```go\n// Named type\ntype EmptyStruct struct{}\n\n// Variable declaration\nvar es struct{}\n\n// Or use it directly!\nstruct{}{}\n```\n\nThe size or width of a `struct` is defined as the sum of its constituent types. An empty `struct`, since it has no fields within it, has a size or width of zero. Zero bytes!\n\nJust like regular structs you can define methods on empty structs as well. Sort of like a zero sized container for methods:\n\n```go\ntype EmptyStruct struct{}\n\nfunc (es *EmptyStruct) WhoAmI() {\n    fmt.Println(\"I am an empty struct!\")\n}\n```\n\nEmpty structs find its best use case in channel signalling. Many a mere mortals such as me have used booleans or integers to notify an event over a channel.\n\nThe following example prints the current time at every 500 milliseconds and times out after 3 seconds,\n\n```go\nfunc printFor3Seconds(doneChannel chan struct{}) {\n    ticker := time.NewTicker(500 * time.Millisecond)\n    timeout := time.After(3 * time.Second)\n\n    for {\n        select {\n        case t := \u003c-ticker.C:\n            fmt.Printf(\"Tik tik: %v\\n\", t)\n        case \u003c-timeout:\n            fmt.Printf(\"Timeout at: %v\\n\", time.Now())\n            doneChannel \u003c- struct{}{}\n            return\n        }\n    }\n}\n\nfunc main() {\n    doneChannel := make(chan struct{}, 1)\n\n    go printFor3Seconds(doneChannel)\n\n    \u003c-doneChannel\n}\n\n// Output\n// $ go run 1.go\n// Tik tik: 2020-09-16 08:25:36.750631 +0530 IST m=+0.501680080\n// Tik tik: 2020-09-16 08:25:37.250303 +0530 IST m=+1.001337269\n// Tik tik: 2020-09-16 08:25:37.750696 +0530 IST m=+1.501714888\n// Tik tik: 2020-09-16 08:25:38.253708 +0530 IST m=+2.004711715\n// Tik tik: 2020-09-16 08:25:38.752804 +0530 IST m=+2.503793180\n// Tik tik: 2020-09-16 08:25:39.251941 +0530 IST m=+3.002914584\n// Timeout at: 2020-09-16 08:25:39.251999 +0530 IST m=+3.002972846\n```\n\nInstead of `doneChannel \u003c- 0` or `doneChannel \u003c- true`, I am using `doneChannel \u003c- struct{}{}` for channel signalling. Using booleans or integers for channel signalling involves memory allocation, copying over the element etc.\n\nWe can use empty structs for simulating sets as well,\n\n```go\nintSet := make(map[int]struct{})\nempty := struct{}{}\n\n// We are setting empty as the value for 1, because the\n// value is meaningless\nintSet[1] = empty\nif _, ok := intSet[1]; ok {\n    fmt.Println(\"1 is in the set\")\n}\n\n// 2 is not one of intSet's keys\nif _, ok := intSet[2]; !ok {\n    fmt.Println(\"2 is not in the set\")\n}\n```\n\n## Resources:\n\n1. [The Go Programming Language Specification](https://golang.org/ref/spec#Struct_types)\n2. [The empty struct - Dave Cheney](https://dave.cheney.net/2014/03/25/the-empty-struct)\n\n---\n\nNote: _This article is not an in-depth tutorial or treatment of Golang's syntax, semantics, design or implementation, but a journal of my learnings._","slug":"4-empty-interface-and-struct"},{"data":{"title":"Sending application logs from EKS to Datadog","date":"2020-08-10T17:04:32+05:30","draft":false,"tags":["architecture"]},"content":"\nimport MDImage from '../../../components/MDImage.js';\n\n# Sending application logs from EKS to Datadog\n\nDatadog has been part of our stack for quite sometime now, where the `datadog-agent` installed on the EC2 instances (baked into the image) tail log files to which the application services write to.\n\nA recent project required me to send the application logs deployed on kubernetes to Datadog. This required a different approach which I would be sharing today. A few pre-requisites before I begin:\n\n1. An up and running kubernetes cluster. I use [`eksctl`](https://eksctl.io/) to quickly spin up a k8s cluster on AWS EKS.\n2. A [Datadog](https://www.datadoghq.com/) account\n3. Helm v3 installed (v3 doesn't require you to setup Tiller (Helm's server component) on your kubernetes cluster)\n\n## Pre-requisites\n\nTo set the context, I have a 3-node cluster set up:\n```sh\n$ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE   VERSION\nip-10-0-0-34.ap-south-1.compute.internal    Ready    \u003cnone\u003e   9d    v1.16.13-eks-2ba888\nip-10-0-1-144.ap-south-1.compute.internal   Ready    \u003cnone\u003e   9d    v1.16.13-eks-2ba888\nip-10-0-2-79.ap-south-1.compute.internal    Ready    \u003cnone\u003e   9d    v1.16.13-eks-2ba888\n``` \n\nAdd and update the Datadog chart repository,\n```sh\n# Ensure that you are running helm v3\n$ helm version --short\nv3.3.1+g249e521\n\n# Add and update the Datadog and stable chart repositories\n$ helm repo add datadog https://helm.datadoghq.com\n\"datadog\" has been added to your repositories\n\n$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/\n\"stable\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"stable\" chart repository\nUpdate Complete. ⎈Happy Helming!⎈\n\n$ helm repo list\nNAME   \tURL                                              \nstable \thttps://kubernetes-charts.storage.googleapis.com/\ndatadog\thttps://helm.datadoghq.com        \n\n# Search for datadog chart. We will be using datadog/datadog\n$ helm search repo datadog\nNAME           \tCHART VERSION\tAPP VERSION\tDESCRIPTION             \ndatadog/datadog\t2.4.13       \t7          \tDatadog Agent           \nstable/datadog \t2.3.42       \t7          \tDEPRECATED Datadog Agent\n```\n\nClone the git repository `bensooraj/node-eks-datadog-integration`:\n```sh\n$ git clone https://github.com/bensooraj/node-eks-datadog-integration.git\n\n# A quick overview overview of the project structure\n$ tree -L 2\n.\n├── Makefile\n├── README.md\n├── coverage\n├── datadog\n│   └── values.yml\n├── docker\n│   └── Dockerfile\n├── kubernetes\n│   ├── deployment.yaml\n│   └── service.yaml\n├── node_modules\n│   ├── @dabh\n│   ....\n│   ...\n│   ..\n│   ├── winston\n│   └── winston-transport\n├── package-lock.json\n├── package.json\n└── server.js\n\n83 directories, 9 files\n```\n\nCheck [datadog-values.yaml](https://github.com/DataDog/helm-charts/blob/master/charts/datadog/values.yaml) for the latest YAML file.\n\n## Deploy the application\n\nWe will be deploying an ultra simple Node.js API which logs out JSON formatted log messages whenever it receives a `GET` request:\n```js\n// server.js\nconst express = require('express');\nconst winston = require('winston');\n\n// Constants\nconst PORT = 8080;\nconst HOST = '0.0.0.0';\n\nconst app = express();\n\nconst logger = winston.createLogger({\n    transports: [\n        new winston.transports.Console({\n            handleExceptions: true,\n        })\n    ],\n    level: 'debug',\n    exitOnError: false,\n    format: winston.format.combine(\n        winston.format.splat(),\n        winston.format.timestamp(),\n        winston.format.json()\n    ),\n});\n\napp.get('/', (req, res) =\u003e {\n    logger.silly(\"This is a silly message\", { url: req.url, environment: \"test\" });\n    logger.debug(\"This is a debug message\", { url: req.url, environment: \"test\" });\n    logger.info(\"This is an info message\", { url: req.url, environment: \"test\" });\n\n    res.json({\n        response_message: 'Hello World'\n    });\n});\n\napp.listen(PORT, HOST);\nlogger.info(\"Running on http://${HOST}:${PORT}\", { host: HOST, port: PORT, environment: \"test\" });\n```\n\nRelevant sections from the deployment YAML file,\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eks-datadog-demo-node-app\n  annotations:\n    ad.datadoghq.com/eks-datadog-demo-node-app.logs: '[{\"source\":\"nodejs\",\"service\":\"eks-datadog-demo-node-app\"}]'  \n  labels:\n    tags.datadoghq.com/env: \"test\"\n    tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n    tags.datadoghq.com/version: \"v1.0.1\"\nspec:\n  # # #\n  # #\n  #\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/eks-datadog-demo-node-app.logs: '[{\"source\":\"nodejs\",\"service\":\"eks-datadog-demo-node-app\"}]'\n      labels:\n        app: eks-datadog-demo-node-app\n        tags.datadoghq.com/env: \"test\"\n        tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n\t\ttags.datadoghq.com/version: \"v1.0.1\"\n\tspec:\n      containers:\n        - name: eks-datadog-demo-node-app\n  # # #\n  # #\n  #\n```\nPay close attention to the `labels` and `annotations`. These are especially useful if you are planning to collect `traces` and application `metrics`, which are outside the scope of this article.\n\nIt's worth mentioning that you can exclude or include logs from deployments and/or pods by configuring the settings in the [`annotations`](https://docs.datadoghq.com/agent/logs/advanced_log_collection/?tab=kubernetes#exclude-at-match). For example, to exclude all logs from the above deployment configure the `log_processing_rules` in the `annotations` section as follows:\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eks-datadog-demo-node-app\n  annotations:\n    ad.datadoghq.com/eks-datadog-demo-node-app.logs: \u003e-\n      [{\n        \"source\": \"nodejs\",\n        \"service\": \"eks-datadog-demo-node-app\",\n        \"log_processing_rules\": [{\n          \"type\": \"exclude_at_match\",\n          \"name\": \"exclude_this_deployment\",\n          \"pattern\":\"\\.*\"\n        }]\n      }]  \n  labels:\n    tags.datadoghq.com/env: \"test\"\n    tags.datadoghq.com/service: \"eks-datadog-demo-node-app\"\n    tags.datadoghq.com/version: \"v1.0.1\"\nspec:\n  # # #\n  # #\n  #\n```\n\nDeploy the API and exopse it as a service,\n```sh\n# Deployment\n$ kubectl apply -f kubernetes/deployment.yaml\n\n# Service\n$ kubectl apply -f kubernetes/service.yaml\n\n# Verify that the API is up and running\n$ kubectl get deployment,svc,po\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/eks-datadog-demo-node-app          2/2     2            2           30h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/eks-datadog-demo-node-app          ClusterIP   172.20.205.150   \u003cnone\u003e        8080/TCP   30h\n\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/eks-datadog-demo-node-app-7bf7cf764f-gpqvf          1/1     Running   0          29h\npod/eks-datadog-demo-node-app-7bf7cf764f-spb7m          1/1     Running   0          29h\n```\n\n## Deploy the Datadog agent\n\nOpen the file `datadog/values.yml` and ensure that `datadog.logs.enabled` and `datadog.logs.containerCollectAll` are set to `true`.\n```yaml\ndatadog:\n  ## @param logs - object - required\n  ## Enable logs agent and provide custom configs\n  #\n  logs:\n    ## @param enabled - boolean - optional - default: false\n    ## Enables this to activate Datadog Agent log collection.\n    #\n    enabled: true\n    ## @param containerCollectAll - boolean - optional - default: false\n    ## Enable this to allow log collection for all containers.\n    #\n    containerCollectAll: true\n```\n\nDeploy the datadog-agent:\n```sh\n$ helm install datadog-agent -f datadog/values.yml  --set datadog.apiKey=\u003cDATADOG_API_KEY\u003e datadog/datadog\n\n# Helm will deploy one datadog-agent pod per node\n$ kubectl get deployment,svc,po\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/datadog-agent-kube-state-metrics   1/1     1            1           29h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/datadog-agent-kube-state-metrics   ClusterIP   172.20.129.97    \u003cnone\u003e        8080/TCP   29h\n\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/datadog-agent-b4mtv                                 2/2     Running   0          29h\npod/datadog-agent-f6jn7                                 2/2     Running   0          29h\npod/datadog-agent-zvp6p                                 2/2     Running   0          29h\n```\n\nNote that even though the agent is deployed in the `default` namespace, it can collect logs and metrics from deployments/pods from all namespaces.\n\n## O Logs! Where art thou?\n\nNow we have the API service (for generating logs) and the Datadog agent (for streaming the logs to Datadog) all set! I assume this is what happens behind the scenes:\n\n1. The API service writes logs to **`stdout`** and **`stderr`**\n2. The **`kubelet`** writes these logs to **`/var/log/pods/\u003cnamespace\u003e_\u003cpod_name\u003e_\u003cpod_id\u003e/\u003ccontainer_name\u003e/\u003cnum\u003e.log`** on the node machine\n3. The **`datadog-agent`**s running on each node tails these log files and streams them to the Datadog servers\n\nAnyways, let's generate some logs:\n```sh\n# Port forward to access the API service from your local machine\n$ kubectl port-forward service/eks-datadog-demo-node-app 8080:8080\nForwarding from 127.0.0.1:8080 -\u003e 8080\nForwarding from [::1]:8080 -\u003e 8080\n\n# Generate some logs using curl or ab\n$ ab -n 50 -c 2 -l http://localhost:8080/\n\n# Verify that the logs are generated\nj$ kubectl logs -l app=eks-datadog-demo-node-app --since=100m\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"debug\",\"message\":\"This is a debug message\",\"timestamp\":\"2020-09-04T19:35:52.126Z\"}\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"info\",\"message\":\"This is an info message\",\"timestamp\":\"2020-09-04T19:35:53.308Z\"}\n....\n...\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"debug\",\"message\":\"This is a debug message\",\"timestamp\":\"2020-09-04T19:35:53.310Z\"}\n{\"url\":\"/\",\"environment\":\"test\",\"level\":\"info\",\"message\":\"This is an info message\",\"timestamp\":\"2020-09-04T19:35:53.310Z\"}\n```\n\nNavigate to `Log Explorer` in Datadog to see the logs that we generated above\n\n\u003cMDImage src=\"/5/2_dd_ui_overview.png\" alt=\"Datadog UI overview\" /\u003e\n\nClick on any one of the log records to view more details\n\n\u003cMDImage src=\"/5/2_dd_log_detail_view.png\" alt=\"Datadog log record detail view\" /\u003e\n\nOn the left pane, you can filter the logs by namespace, pods etc. as well!\n\n\u003cMDImage src=\"/5/2_filter_ui_ns.png\" alt=\"Datadog log record detail view UI\" /\u003e\n\nThat's it for now!\n\nI hope this helped and if you have any feedback for me, please let me know :smile:.\n\nFurther reading:\n1. [Logging Architecture](https://kubernetes.io/docs/concepts/cluster-administration/logging/)\n2. [Datadog kubernetes log collection](https://docs.datadoghq.com/agent/kubernetes/log/?tab=daemonset#log-collection)\n3. [Datadog agent as a DaemonSet (recommended)](https://docs.datadoghq.com/agent/kubernetes/?tab=daemonset)\n---\n\nNote: _This article is not an in-depth tutorial on implemending logging for applications running on kubernetes, but a journal of my learnings._","slug":"5-eks-datadog"},{"data":{"title":"Struct aggregation and embedding in golang","date":"2020-08-03T02:04:32+05:30","tags":["golang"]},"content":"\n# Struct aggregation and embedding in golang\n\nA **struct**,\n- is a collection of fields of various types, incuding other structs!\n- can be used to create user-defined data types\n- can have methods defined on them\n- its fields and methods are accessed using the **dot** notation\n\nFor example, `Point` is a struct which has,\n\n- two fields `X` and `Y` of type `float64`\n- has two methods `String` and `MakesNoSense`\n\n```go\n// structs.go\n// Struct definition\ntype Point struct {\n\tX, Y float64\n}\n\n// String overrides how an entity of type `Point` \n// is printed (for example, to `stdout`)\nfunc (p Point) String() string {\n\treturn fmt.Sprintf(\"Point {%f, %f}\", p.X, p.Y)\n}\n\n// MakesNoSense is just a dummy method\nfunc (p Point) MakesNoSense() {\n\tfmt.Printf(\"Point {%f, %f} is just a point!\\n\", p.X, p.Y)\n\treturn\n}\n\nfunc main() {\n\tvar pt Point\n\tpt.X = 1.123 // \n\tpt.Y = 2.123 // \n\n    fmt.Println(pt)\n    pt.MakesNoSense()\n}\n```\n\nresults in,\n```sh\n$ go run structs.go\nPoint {1.123000, 2.123000}\nPoint {1.123000, 2.123000} is just a point!\n```\n\n## Nested Structs\n\nYes, you can define one struct inside another. There are two ways to go about it: **struct aggregation** and **struct embedding**.\n\n### Struct Aggregation\n\nThis approach implies a `has-a` relationship. For example, a line has two points (start and end) and can be declared as follows:\n\n```go\n// Line struct has two fields of type Point\ntype Line struct {\n\tStart, End Point\n}\n\n// Distance methods calculates the euclidean distance \n// between the two Points\nfunc (l Line) Distance() float64 {\n\txDiff := math.Abs(l.Start.X - l.End.X)\n\tyDiff := math.Abs(l.Start.Y - l.End.Y)\n\n\treturn math.Sqrt(math.Pow(xDiff, 2) + math.Pow(yDiff, 2))\n}\n\n// Usage\nfunc main() {\n\tl := Line{\n\t\tStart: Point{\n\t\t\tX: 2.304,\n\t\t\tY: 4.504,\n\t\t},\n\t\tEnd: Point{\n\t\t\tX: 30.607,\n\t\t\tY: 44.104,\n\t\t},\n\t}\n\tfmt.Printf(\"Distance from %v to %v is %f units\\n\", l.Start, l.End, l.Distance())\n}\n// Distance from Point {2.304000, 4.504000} to Point {30.607000, 44.104000} is 48.674632 units\n```\n\nLet's redefine `Line` struct slightly differently using inline structs,\n```go\ntype Line struct {\n\tStart, End struct {\n\t\tX float64\n\t\tY float64\n\t}\n}\n\n// Usage\nfunc main() {\n\tl := Line{\n\t\tStart: struct {\n\t\t\tX float64\n\t\t\tY float64\n\t\t}{\n\t\t\tX: 3.123,\n\t\t\tY: 8.123,\n\t\t},\n\t\tEnd: struct {\n\t\t\tX float64\n\t\t\tY float64\n\t\t}{\n\t\t\tX: 4.123,\n\t\t\tY: 7.123,\n\t\t},\n\t}\n}\n```\nThis approach requires you to rely on `anonymous structs` during initialization.\n\n### Struct Embedding\n\nThis approach implies an `is-a` relationship. For example, a rectangle is a polygon and can be decalred as shown below:\n```go\n// embed.go\n// Polygon has just two fields for the sake of simplicity\ntype Polygon struct {\n\tWidth, Height int\n}\n\n// Set width and height of the polygon\nfunc (p *Polygon) Set(w, h int) {\n\tp.Width = w\n\tp.Height = h\n}\n\n// Rectangle is a polygon, with one extra field 'color'\ntype Rectangle struct {\n\tcolor string\n\tPolygon // Notice the embedding?\n}\n\n// Area method can access the fields Width and Height even though\n// they are not directly defined within the Rectangle struct\nfunc (r *Rectangle) Area() float64 {\n\treturn float64(r.Width * r.Height)\n}\n\nfunc main() {\n\tvar rect Rectangle\n\trect.Set(10, 20) // direct\n\trect.color = \"Blue\"\n\tfmt.Printf(\"Rectangle: %+v\\n\", rect)\n\tfmt.Printf(\"Rectangle Width: %+v\\n\", rect.Width) // direct\n\tfmt.Printf(\"Rectangle Height: %+v\\n\", rect.Height) // direct\n\tfmt.Printf(\"Area of the rectangle is: %+v\\n\", rect.Area())\n\n\trect.Polygon.Set(100, 200) // indirect\n\tfmt.Printf(\"Rectangle: %+v\\n\", rect)\n}\n``` \nresults in,\n```sh\n$ go run embed.go\nRectangle: {color:Blue Polygon:{Width:10 Height:20}}\nRectangle Width: 10\nRectangle Height: 20\nArea of the rectangle is: 200\nRectangle: {color:Blue Polygon:{Width:100 Height:200}}\n```\n\nYou can see that `rect` can access:\n1. `Rectangle` struct's fields and methods\n2. `Polygon` struct's fields and methods - both directly and indirectly\n\nI am not sure if that's the correct way to put it, but these are my observations.\n\n---\n\nNote: _This article is not an in-depth tutorial or treatment of Golang's syntax, semantics, design or implementation, but a journal of my learnings._","slug":"3-go-struct-aggregation-and-embedding"},{"data":{"title":"Up And Running With Kafka On AWS EKS Using Strimzi","date":"2019-10-09T09:00:32+05:30","tags":["kubernetes","aws"]},"content":"\nimport MDImage from '../../../components/MDImage.js';\nimport BackToTop from '../../../components/BackToTop.js';\n\n# Up And Running With Kafka On AWS EKS Using Strimzi\n\n## Contents \u003ca id=\"contents\"\u003e\u003c/a\u003e\n\n1. [Configure the AWS CLI](#configure-the-aws-cli)\n2. [Create the EKS cluster](#create-the-eks-cluster)\n3. [Enter Kubernetes](#enter-k8s)\n4. [Install and configure Helm](#install-and-configure-helm)\n5. [Install the Strimzi Kafka Operator](#install-strimzi-kafka-operator)\n6. [Deploying the Kafka cluster](#deploy-kaka-cluster)\n7. [Analysis](#analysis)\n8. [Create topics](#create-topics)\n9. [Test the Kafka cluster with Node.js clients](#test-kafka-cluster)\n10. [Clean up!](#clean-up)\n\nLet's get right into it, then!\n\nWe will be using [`eksctl`][1], the official CLI for Amazon EKS, to spin up our K8s cluster.\n\n## Configure the AWS CLI \u003ca name=\"configure-the-aws-cli\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\nEnsure that the AWS CLI is [configured][2]. To view your configuration:\n\n```go\n$ aws configure list\n      Name                    Value             Type    Location\n      ----                    -----             ----    --------\n   profile                \u003cnot set\u003e             None    None\naccess_key     ****************7ONG shared-credentials-file    \nsecret_key     ****************lbQg shared-credentials-file    \n    region               ap-south-1      config-file    ~/.aws/config\n```\n\nNote: The aws CLI config and credentials details are usually stored at `~/.aws/config` and `~/.aws/credentials` respectively.\n\n\n## Create the EKS cluster \u003ca name=\"create-the-eks-cluster\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\n```\n\n$ eksctl create cluster --name=kafka-eks-cluster --nodes=4 --region=ap-south-1\n\n[ℹ]  using region ap-south-1\n[ℹ]  setting availability zones to [ap-south-1b ap-south-1a ap-south-1c]\n[ℹ]  subnets for ap-south-1b - public:192.168.0.0/19 private:192.168.96.0/19\n[ℹ]  subnets for ap-south-1a - public:192.168.32.0/19 private:192.168.128.0/19\n[ℹ]  subnets for ap-south-1c - public:192.168.64.0/19 private:192.168.160.0/19\n[ℹ]  nodegroup \"ng-9f3cbfc7\" will use \"ami-09c3eb35bb3be46a4\" [AmazonLinux2/1.12]\n[ℹ]  creating EKS cluster \"kafka-eks-cluster\" in \"ap-south-1\" region\n[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup\n[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-south-1 --name=kafka-eks-cluster'\n[ℹ]  2 sequential tasks: { create cluster control plane \"kafka-eks-cluster\", create nodegroup \"ng-9f3cbfc7\" }\n[ℹ]  building cluster stack \"eksctl-kafka-eks-cluster-cluster\"\n[ℹ]  deploying stack \"eksctl-kafka-eks-cluster-cluster\"\n[ℹ]  building nodegroup stack \"eksctl-kafka-eks-cluster-nodegroup-ng-9f3cbfc7\"\n[ℹ]  --nodes-min=4 was set automatically for nodegroup ng-9f3cbfc7\n[ℹ]  --nodes-max=4 was set automatically for nodegroup ng-9f3cbfc7\n[ℹ]  deploying stack \"eksctl-kafka-eks-cluster-nodegroup-ng-9f3cbfc7\"\n[✔]  all EKS cluster resource for \"kafka-eks-cluster\" had been created\n[✔]  saved kubeconfig as \"/Users/Bensooraj/.kube/config\"\n[ℹ]  adding role \"arn:aws:iam::account_numer:role/eksctl-kafka-eks-cluster-nodegrou-NodeInstanceRole-IG63RKPE03YQ\" to auth ConfigMap\n[ℹ]  nodegroup \"ng-9f3cbfc7\" has 0 node(s)\n[ℹ]  waiting for at least 4 node(s) to become ready in \"ng-9f3cbfc7\"\n[ℹ]  nodegroup \"ng-9f3cbfc7\" has 4 node(s)\n[ℹ]  node \"ip-192-168-25-34.ap-south-1.compute.internal\" is ready\n[ℹ]  node \"ip-192-168-50-249.ap-south-1.compute.internal\" is ready\n[ℹ]  node \"ip-192-168-62-231.ap-south-1.compute.internal\" is ready\n[ℹ]  node \"ip-192-168-69-95.ap-south-1.compute.internal\" is ready\n[ℹ]  kubectl command should work with \"/Users/Bensooraj/.kube/config\", try 'kubectl get nodes'\n[✔]  EKS cluster \"kafka-eks-cluster\" in \"ap-south-1\" region is ready\n\n```\n\nA k8s cluster by the name **kafka-eks-cluster** will be created with 4 nodes (instance type: [m5.large][3]) in the Mumbai region (ap-south-1). You can view these in the AWS Console UI as well,\n\nEKS:\n\u003cMDImage src=\"/2/1_xamksw0rnxsxjohb8zkh.png\" alt=\"AWS EKS UI\" /\u003e\n\nCloudFormation UI:\n\u003cMDImage src=\"/2/2_8gj6kw97f6bxkm6u9a44.png\" alt=\"Cloudformation UI\" /\u003e\n\nAlso, after the cluster is created, the appropriate kubernetes configuration will be added to your kubeconfig file (defaults to `~/.kube/config`). The path to the kubeconfig file can be overridden using the `--kubeconfig` flag.\n\n\n## Enter Kubernetes \u003ca name=\"enter-k8s\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nFetching all k8s controllers lists the default `kubernetes` service. This confirms that `kubectl` is properly configured to point to the cluster that we just created.\n\n```sh\n$ kubectl get all\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.100.0.1   \u003cnone\u003e        443/TCP   19m\n```\n\n## Install and configure Helm \u003ca name=\"install-and-configure-helm\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\n\u003e [Helm][4] is a package manager and application management tool for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\n\nI use *Homebrew*, so the installation was pretty straightforward: `brew install kubernetes-helm`.\n\nAlternatively, to install `helm`, run the following:\n\n```sh\n$ cd ~/eks-kafka-strimzi\n\n$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u003e get_helm.sh\n\n$ chmod +x get_helm.sh\n\n$ ./get_helm.sh\n```\n\nRead through their [installation guide][5], if you are looking for more options.\n\n**Do not run `helm init` yet.**\n\n`Helm` relies on a service called **`tiller`** that requires special permission on the kubernetes cluster, so we need to build a **`Service Account`** (RBAC access) for **`tiller`** to use.\n\nThe `rbac.yaml` file would look like the following:\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system\n```\n\nApply this to the `kafka-eks-cluster` cluster:\n```sh\n$ kubectl apply -f rbac.yaml\nserviceaccount/tiller created\nclusterrolebinding.rbac.authorization.k8s.io/tiller created\n\n# Verify (listing only the relevant ones)\n$ kubectl get sa,clusterrolebindings --namespace=kube-system\nNAME                        SECRETS   AGE\n.\nserviceaccount/tiller       1         5m22s\n.\n\nNAME                                                                                                AGE\n.\nclusterrolebinding.rbac.authorization.k8s.io/tiller                                                 5m23s\n.\n```\n\nNow, run **`helm init`** using the service account we setup. This will install tiller into the cluster which gives it access to manage resources in your cluster.\n```sh\n$ helm init --service-account=tiller\n\n$HELM_HOME has been configured at /Users/Bensooraj/.helm.\n\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n\nPlease note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\n\nTo prevent this, run `helm init` with the --tiller-tls-verify flag.\n\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n```\n\n## Install the Strimzi Kafka Operator \u003ca name=\"install-strimzi-kafka-operator\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nAdd the Strimzi repository and install the Strimzi Helm Chart:\n```sh\n# Add the repo\n$ helm repo add strimzi http://strimzi.io/charts/\n\"strimzi\" has been added to your repositories\n\n# Search for all Strimzi  charts\n$ helm search strim\nNAME                          \tCHART VERSION\tAPP VERSION\tDESCRIPTION                \nstrimzi/strimzi-kafka-operator\t0.14.0       \t0.14.0     \tStrimzi: Kafka as a Service\n\n# Install the kafka operator\n$ helm install strimzi/strimzi-kafka-operator\nNAME:   bulging-gnat\nLAST DEPLOYED: Wed Oct  2 15:23:45 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==\u003e v1/ClusterRole\nNAME                                 AGE\nstrimzi-cluster-operator-global      0s\nstrimzi-cluster-operator-namespaced  0s\nstrimzi-entity-operator              0s\nstrimzi-kafka-broker                 0s\nstrimzi-topic-operator               0s\n\n==\u003e v1/ClusterRoleBinding\nNAME                                              AGE\nstrimzi-cluster-operator                          0s\nstrimzi-cluster-operator-kafka-broker-delegation  0s\n\n==\u003e v1/Deployment\nNAME                      READY  UP-TO-DATE  AVAILABLE  AGE\nstrimzi-cluster-operator  0/1    1           0          0s\n\n==\u003e v1/Pod(related)\nNAME                                       READY  STATUS             RESTARTS  AGE\nstrimzi-cluster-operator-6667fbc5f8-cqvdv  0/1    ContainerCreating  0         0s\n\n==\u003e v1/RoleBinding\nNAME                                                 AGE\nstrimzi-cluster-operator                             0s\nstrimzi-cluster-operator-entity-operator-delegation  0s\nstrimzi-cluster-operator-topic-operator-delegation   0s\n\n==\u003e v1/ServiceAccount\nNAME                      SECRETS  AGE\nstrimzi-cluster-operator  1        0s\n\n==\u003e v1beta1/CustomResourceDefinition\nNAME                                AGE\nkafkabridges.kafka.strimzi.io       0s\nkafkaconnects.kafka.strimzi.io      0s\nkafkaconnects2is.kafka.strimzi.io   0s\nkafkamirrormakers.kafka.strimzi.io  0s\nkafkas.kafka.strimzi.io             1s\nkafkatopics.kafka.strimzi.io        1s\nkafkausers.kafka.strimzi.io         1s\n\nNOTES:\nThank you for installing strimzi-kafka-operator-0.14.0\n\nTo create a Kafka cluster refer to the following documentation.\n\nhttps://strimzi.io/docs/0.14.0/#kafka-cluster-str\n```\n\nList all the kubernetes objects created again:\n```sh\n$ kubectl get all\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/strimzi-cluster-operator-6667fbc5f8-cqvdv   1/1     Running   0          9m25s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.100.0.1   \u003cnone\u003e        443/TCP   90m\n\nNAME                                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/strimzi-cluster-operator   1         1         1            1           9m25s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/strimzi-cluster-operator-6667fbc5f8   1         1         1       9m26s\n```\n\n## Deploying the Kafka cluster \u003ca name=\"deploy-kaka-cluster\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nWe will now create a Kafka cluster with 3 brokers. The YAML file (`kafka-cluster.Kafka.yaml`) for creating the Kafka cluster would like the following:\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\nkind: Kafka\nmetadata:\n  name: kafka-cluster\nspec:\n  kafka:\n    version: 2.3.0 # Kafka version\n    replicas: 3 # Replicas specifies the number of broker nodes.\n    listeners: # Listeners configure how clients connect to the Kafka cluster\n      plain: {} # 9092\n      tls: {} # 9093\n    config:\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      log.message.format.version: \"2.3\"\n      delete.topic.enable: \"true\"\n    storage:\n      type: persistent-claim\n      size: 10Gi\n      deleteClaim: false\n  zookeeper:\n    replicas: 3\n    storage:\n      type: persistent-claim # Persistent storage backed by AWS EBS\n      size: 10Gi\n      deleteClaim: false\n  entityOperator:\n    topicOperator: {} # Operator for topic administration\n    userOperator: {}\n\n```\n\nApply the above YAML file:\n```sh\n$ kubectl apply -f kafka-cluster.Kafka.yaml\n```\n\n## Analysis \u003ca name=\"analysis\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nThis is where things get interesting. We will now analyse **some** of the k8s resources which the `strimzi kafka operator` has created for us under the hood.\n\n```sh\n$ kubectl get statefulsets.apps,pod,deployments,svc\nNAME                                       DESIRED   CURRENT   AGE\nstatefulset.apps/kafka-cluster-kafka       3         3         78m\nstatefulset.apps/kafka-cluster-zookeeper   3         3         79m\n\nNAME                                                 READY   STATUS    RESTARTS   AGE\npod/kafka-cluster-entity-operator-54cb77fd9d-9zbcx   3/3     Running   0          77m\npod/kafka-cluster-kafka-0                            2/2     Running   0          78m\npod/kafka-cluster-kafka-1                            2/2     Running   0          78m\npod/kafka-cluster-kafka-2                            2/2     Running   0          78m\npod/kafka-cluster-zookeeper-0                        2/2     Running   0          79m\npod/kafka-cluster-zookeeper-1                        2/2     Running   0          79m\npod/kafka-cluster-zookeeper-2                        2/2     Running   0          79m\npod/strimzi-cluster-operator-6667fbc5f8-cqvdv        1/1     Running   0          172m\n\nNAME                                                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/kafka-cluster-entity-operator   1         1         1            1           77m\ndeployment.extensions/strimzi-cluster-operator        1         1         1            1           172m\n\nNAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/kafka-cluster-kafka-bootstrap    ClusterIP   10.100.177.177   \u003cnone\u003e        9091/TCP,9092/TCP,9093/TCP   78m\nservice/kafka-cluster-kafka-brokers      ClusterIP   None             \u003cnone\u003e        9091/TCP,9092/TCP,9093/TCP   78m\nservice/kafka-cluster-zookeeper-client   ClusterIP   10.100.199.128   \u003cnone\u003e        2181/TCP                     79m\nservice/kafka-cluster-zookeeper-nodes    ClusterIP   None             \u003cnone\u003e        2181/TCP,2888/TCP,3888/TCP   79m\nservice/kubernetes                       ClusterIP   10.100.0.1       \u003cnone\u003e        443/TCP                      4h13m\n```\n\nPoints to note:\n1. The StatefulSet `kafka-cluster-zookeeper` has created 3 pods - `kafka-cluster-zookeeper-0`, `kafka-cluster-zookeeper-1` and `kafka-cluster-zookeeper-2`. The headless service `kafka-cluster-zookeeper-nodes` facilitates network identity of these 3 pods (the 3 Zookeeper nodes).\n2. The StatefulSet `kafka-cluster-kafka` has created 3 pods - `kafka-cluster-kafka-0`, `kafka-cluster-kafka-1` and `kafka-cluster-kafka-2`. The headless service `kafka-cluster-kafka-brokers` facilitates network identity of these 3 pods (the 3 Kafka brokers).\n\nPersistent volumes are dynamically provisioned:\n```sh\n$ kubectl get pv,pvc\nNAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                    STORAGECLASS   REASON   AGE\npersistentvolume/pvc-7ff2909f-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-zookeeper-1   gp2                     11h\npersistentvolume/pvc-7ff290c4-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-zookeeper-2   gp2                     11h\npersistentvolume/pvc-7ffd1d22-e507-11e9-a775-029ce0835b96   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-zookeeper-0   gp2                     11h\npersistentvolume/pvc-a5997b77-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-kafka-0       gp2                     11h\npersistentvolume/pvc-a599e52b-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-kafka-1       gp2                     11h\npersistentvolume/pvc-a59c6cd2-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            Delete           Bound    default/data-kafka-cluster-kafka-2       gp2                     11h\n\nNAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/data-kafka-cluster-kafka-0       Bound    pvc-a5997b77-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            gp2            11h\npersistentvolumeclaim/data-kafka-cluster-kafka-1       Bound    pvc-a599e52b-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            gp2            11h\npersistentvolumeclaim/data-kafka-cluster-kafka-2       Bound    pvc-a59c6cd2-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            gp2            11h\npersistentvolumeclaim/data-kafka-cluster-zookeeper-0   Bound    pvc-7ffd1d22-e507-11e9-a775-029ce0835b96   10Gi       RWO            gp2            11h\npersistentvolumeclaim/data-kafka-cluster-zookeeper-1   Bound    pvc-7ff2909f-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            gp2            11h\npersistentvolumeclaim/data-kafka-cluster-zookeeper-2   Bound    pvc-7ff290c4-e507-11e9-91df-0a1e73fdd786   10Gi       RWO            gp2            11h\n```\n\nYou can view the provisioned AWS EBS volumes in the UI as well:\n\u003cMDImage src=\"/2/3_zys2wmubcb42glzzp23m.png\" alt=\"EBS UI\" /\u003e\n\n## Create topics \u003ca name=\"create-topics\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nBefore we get started with clients we need to create a **topic** (with 3 partitions and a replication factor of 3), over which our `producer` and the `consumer` and produce messages and consume messages on respectively.\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: test-topic\n  labels:\n    strimzi.io/cluster: kafka-cluster\nspec:\n  partitions: 3\n  replicas: 3\n```\n\nApply the YAML to the k8s cluster:\n\n```sh\n$ kubectl apply -f create-topics.yaml\nkafkatopic.kafka.strimzi.io/test-topic created\n```\n\n## Test the Kafka cluster with Node.js clients \u003ca name=\"test-kafka-cluster\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\nThe multi-broker Kafka cluster that we deployed is backed by `statefulset`s and their corresponding headless `service`s.\n\nSince each Pod (Kafka broker) now has a network identity, clients can connect to the Kafka brokers via a combination of the pod name and service name: `$(podname).$(governing service domain)`. In our case, these would be the following URLs:\n1. `kafka-cluster-kafka-0.kafka-cluster-kafka-brokers`\n2. `kafka-cluster-kafka-1.kafka-cluster-kafka-brokers`\n3. `kafka-cluster-kafka-2.kafka-cluster-kafka-brokers`\n\n**Note**:\n1. If the Kafka cluster is deployed in a different namespace, you will have to expand it a little further: `$(podname).$(service name).$(namespace).svc.cluster.local`.\n2. Alternatively, the clients can connect to the Kafka cluster using the Service `kafka-cluster-kafka-bootstrap:9092` as well. It distributes the connection over the three broker specific endpoints I have listed above. As I no longer keep track of the individual broker endpoints, this method plays out well when I have to scale up or down the number of brokers in the Kafka cluster.\n\nFirst, clone this repo: [strimzi-kafka-aws-eks](https://github.com/bensooraj/strimzi-kafka-aws-eks)\n```sh\n# Create the configmap, which contains details such as the broker DNS names, topic name and consumer group ID\n$ kubectl apply -f test/k8s/config.yaml\nconfigmap/kafka-client-config created\n\n# Create the producer deployment\n$ kubectl apply -f test/k8s/producer.Deployment.yaml\ndeployment.apps/node-test-producer created\n\n# Expose the producer deployment via a service of type LoadBalancer (backed by the AWS Elastic Load Balancer). This just makes it easy for me to curl from postman\n$ kubectl apply -f test/k8s/producer.Service.yaml\nservice/node-test-producer created\n\n# Finally, create the consumer deployment\n$ kubectl apply -f test/k8s/consumer.Deployment.yaml\ndeployment.apps/node-test-consumer created\n\n```\n\nIf you list the producer service that we created, you would notice a `URL` under EXTERNAL-IP:\n```sh\n$ kubectl get svc\nNAME                             TYPE           CLUSTER-IP       EXTERNAL-IP                                                                PORT(S)                      AGE\n.\n.\nnode-test-producer               LoadBalancer   10.100.145.203   ac5f3d0d1e55a11e9a775029ce0835b9-2040242746.ap-south-1.elb.amazonaws.com   80:31231/TCP                 55m\n\n```\n\nThe URL `ac5f3d0d1e55a11e9a775029ce0835b9-2040242746.ap-south-1.elb.amazonaws.com` is an `AWS ELB` backed public endpoint which we will be querying for producing messages to the Kafka cluster.\n\nAlso, you can see that there is 1 producer and 3 consumers (one for each partition of the topic `test-topic`):\n```sh\n$ kubectl get pod\nNAME                                             READY   STATUS    RESTARTS   AGE\nnode-test-consumer-96b44cbcb-gs2km               1/1     Running   0          125m\nnode-test-consumer-96b44cbcb-ptvjd               1/1     Running   0          125m\nnode-test-consumer-96b44cbcb-xk75j               1/1     Running   0          125m\nnode-test-producer-846d9c5986-vcsf2              1/1     Running   0          125m\n```\n\nThe producer app basically exposes 3 URLs:\n1. `/kafka-test/green/:message`\n2. `/kafka-test/blue/:message`\n3. `/kafka-test/cyan/:message`\n\nWhere `:message` can be any valid string. Each of these URLs produce a **message** along with the **colour** information to the topic `test-topic`.\n\nThe consumer group (the 3 consumer pods that we spin-up) listening for any incoming messages from the topic `test-topic`, then receives these messages and prints them on to the console according to the colour instruction.\n\nI `curl` each URL 3 times. From the following GIF you can see how message consumption is distributed across the 3 consumers in a `round-robin` manner:\n\n\u003cMDImage src=\"/2/4_a3b19iryt7pxff3z8ust.gif\" alt=\"Producer and Consumer Visualisation\" /\u003e\n\n## Clean Up! \u003ca name=\"clean-up\"\u003e\u003c/a\u003e \u003cBackToTop /\u003e\n\n```sh\n\n# Delete the test producer and consumer apps:\n$ kubectl delete -f test/k8s/\nconfigmap \"kafka-client-config\" deleted\ndeployment.apps \"node-test-consumer\" deleted\ndeployment.apps \"node-test-producer\" deleted\nservice \"node-test-producer\" deleted\n\n# Delete the Kafka cluster\n$ kubectl delete kafka kafka-cluster\nkafka.kafka.strimzi.io \"kafka-cluster\" deleted\n\n# Delete the Strimzi cluster operator\n$ kubectl delete deployments. strimzi-cluster-operator\ndeployment.extensions \"strimzi-cluster-operator\" deleted\n\n# Manually delete the persistent volumes\n# Kafka\n$ kubectl delete pvc data-kafka-cluster-kafka-0\n$ kubectl delete pvc data-kafka-cluster-kafka-1\n$ kubectl delete pvc data-kafka-cluster-kafka-2\n# Zookeeper\n$ kubectl delete pvc data-kafka-cluster-zookeeper-0\n$ kubectl delete pvc data-kafka-cluster-zookeeper-1\n$ kubectl delete pvc data-kafka-cluster-zookeeper-2\n```\n\nFinally, delete the EKS cluster:\n```sh\n$ eksctl delete cluster kafka-eks-cluster\n[ℹ]  using region ap-south-1\n[ℹ]  deleting EKS cluster \"kafka-eks-cluster\"\n[✔]  kubeconfig has been updated\n[ℹ]  2 sequential tasks: { delete nodegroup \"ng-9f3cbfc7\", delete cluster control plane \"kafka-eks-cluster\" [async] }\n[ℹ]  will delete stack \"eksctl-kafka-eks-cluster-nodegroup-ng-9f3cbfc7\"\n[ℹ]  waiting for stack \"eksctl-kafka-eks-cluster-nodegroup-ng-9f3cbfc7\" to get deleted\n[ℹ]  will delete stack \"eksctl-kafka-eks-cluster-cluster\"\n[✔]  all cluster resources were deleted\n```\n\nHope this helped!\n\n[1]: https://eksctl.io/\n[2]: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n[3]: https://aws.amazon.com/ec2/instance-types/\n[4]: https://helm.sh\n[5]: https://helm.sh/docs/using_helm/#installing-helm\n[6]: https://github.com/bensooraj/strimzi-kafka-aws-eks\n\n---\n\nNote: _This is not a tutorial per se, instead, this is me recording my observations as I setup a Kafka cluster for the first time on a Kubernetes platform using Strimzi._","slug":"2-up-and-running-with-kafka-on-aws-eks-using-strimzi"},{"data":{"title":"Accessing Amazon RDS From AWS EKS","date":"2019-10-03T07:00:32+05:30","tags":["kubernetes","aws"]},"content":"\nimport MDImage from '../../../components/MDImage.js';\n\n# Accessing Amazon RDS From AWS EKS\n\n## Contents \u003ca id=\"contents\"\u003e\u003c/a\u003e\n\n1.  [Premise](#premise)\n2.  [Setup the MySQL Database - Amazon RDS](#setup-the-mysql-database)\n    1.  [Create the VPC](#rds-create-the-vpc)\n    2.  [Create the subnets](#rds-create-subnets)\n    3.  [Create the DB subnet group](#rds-create-db-subnet-group)\n    4.  [Create the VPC security group](#rds-create-vpc-security-group)\n    5.  [Create a DB instance in the VPC](#rds-create-db-instance-in-the-vpc)\n    6.  [Amazon RDS setup diagram](#rds-setup-diagram)\n3.  [Setup the EKS cluster](#setup-the-eks-cluster)\n4.  [Let's build the bridge!](#lets-build-the-bridge)\n    1.  [Create and Accept a VPC Peering Connection](#create-and-accept-vpc-peering-connections)\n    2.  [Update the EKS cluster VPC's route table](#update-eks-cluster-vpc-route-table)\n    3.  [Update the RDS VPC's route table](#update-rds-vpc-route-table)\n    4.  [Update the RDS instance's security group](#update-rds-instance-security-group)\n5.  [Test the connection](#test-the-connection)\n\n## 1. Premise \u003ca id=\"premise\"\u003e\u003c/a\u003e\n\nWhen moving your services to the Kubernetes ecosystem for the first time, it is best practice to port only the stateless parts to begin with.\n\nHere's the problem I had to solve: Our service uses [Amazon RDS for MySQL][1]. Both the RDS instance(s) and EKS reside within their own dedicated [VPC][2]. How do resources running within AWS EKS communicate with the database?\n\n\u003cMDImage src=\"/0/1_9f54sopkrm2za71fhvl5.png\" alt=\"Problem visualised\" /\u003e\n\nLet's dive right in!\n\n## 2. Setup the MySQL Database (Amazon RDS) \u003ca id=\"setup-the-mysql-database\"\u003e\u003c/a\u003e\n\nWe will be using the AWS CLI for setting up MySQL database.\n\n### 2.1 Create the VPC \u003ca id=\"rds-create-the-vpc\"\u003e\u003c/a\u003e\n\nWe will first create a VPC with the CIDR block `10.0.0.0/24` which accommodate 254 hosts in all. This is **more than enough** to host our RDS instance.\n\n```sh\n$ aws ec2 create-vpc --cidr-block 10.0.0.0/24 | jq '{VpcId:.Vpc.VpcId,CidrBlock:.Vpc.CidrBlock}'\n{\n    \"VpcId\": \"vpc-0cf40a5f6db5eb3cd\",\n    \"CidrBlock\": \"10.0.0.0/24\"\n}\n\n# Export the RDS VPC ID for easy reference in the subsequent commands\n$ export RDS_VPC_ID=vpc-0cf40a5f6db5eb3cd\n```\n\n### 2.2 Create the subnets \u003ca id=\"rds-create-subnets\"\u003e\u003c/a\u003e\n\nRDS instances launched in a VPC must have a [DB subnet group][3]. DB subnet groups are a collection of subnets within a VPC. Each DB subnet group should have `subnets` in at least two `Availability Zones` in a given `AWS Region`.\n\nWe will divide the RDS VPC (`RDS_VPC_ID`) into two equal subnets: `10.0.0.0/25` and `10.0.0.128/25`.\n\nSo, let's create the first subnet in the availability zone `ap-south-1b`:\n\n```sh\n$ aws ec2 create-subnet --availability-zone \"ap-south-1b\" --vpc-id ${RDS_VPC_ID} --cidr-block 10.0.0.0/25 | jq '{SubnetId:.Subnet.SubnetId,AvailabilityZone:.Subnet.AvailabilityZone,CidrBlock:.Subnet.CidrBlock,VpcId:.Subnet.VpcId}'\n# Response:\n{\n  \"SubnetId\": \"subnet-042a4bee8e92287e8\",\n  \"AvailabilityZone\": \"ap-south-1b\",\n  \"CidrBlock\": \"10.0.0.0/25\",\n  \"VpcId\": \"vpc-0cf40a5f6db5eb3cd\"\n}\n\n```\n\nand the second one in the availability zone `ap-south-1a`\n\n```sh\n$ aws ec2 create-subnet --availability-zone \"ap-south-1a\" --vpc-id ${RDS_VPC_ID} --cidr-block 10.0.0.128/25 | jq '{SubnetId:.Subnet.SubnetId,AvailabilityZone:.Subnet.AvailabilityZone,CidrBlock:.Subnet.CidrBlock,VpcId:.Subnet.VpcId}'\n# Response:\n{\n  \"SubnetId\": \"subnet-0c01a5ba480b930f4\",\n  \"AvailabilityZone\": \"ap-south-1a\",\n  \"CidrBlock\": \"10.0.0.128/25\",\n  \"VpcId\": \"vpc-0cf40a5f6db5eb3cd\"\n}\n```\n\nEach VPC has an implicit router which controls where network traffic is directed. Each subnet in a VPC must be explicitly associated with a route table, which controls the routing for the subnet.\n\nLet's go ahead and associate these two subnet that we created, to the VPC's route table:\n\n```sh\n# Fetch the route table information\n$ aws ec2 describe-route-tables --filters Name=vpc-id,Values=${RDS_VPC_ID} | jq '.RouteTables[0].RouteTableId'\n\"rtb-0e680357de97595b1\"\n\n# For easy reference\n$ export RDS_ROUTE_TABLE_ID=rtb-0e680357de97595b1\n\n# Associate the first subnet with the route table\n$ aws ec2 associate-route-table --route-table-id rtb-0e680357de97595b1 --subnet-id subnet-042a4bee8e92287e8\n{\n    \"AssociationId\": \"rtbassoc-02198db22b2d36c97\"\n}\n\n# Associate the second subnet with the route table\n$ aws ec2 associate-route-table --route-table-id rtb-0e680357de97595b1 --subnet-id subnet-0c01a5ba480b930f4\n{\n    \"AssociationId\": \"rtbassoc-0e5c3959d360c92ab\"\n}\n\n```\n\n### 2.3 Create DB Subnet Group \u003ca id=\"rds-create-db-subnet-group\"\u003e\u003c/a\u003e\n\nNow that we have two subnets spanning two availability zones, we can go ahead and create the **DB subnet group**.\n\n```sh\n$ aws rds create-db-subnet-group --db-subnet-group-name  \"DemoDBSubnetGroup\" --db-subnet-group-description \"Demo DB Subnet Group\" --subnet-ids \"subnet-042a4bee8e92287e8\" \"subnet-0c01a5ba480b930f4\" | jq '{DBSubnetGroupName:.DBSubnetGroup.DBSubnetGroupName,VpcId:.DBSubnetGroup.VpcId,Subnets:.DBSubnetGroup.Subnets[].SubnetIdentifier}'\n# Response:\n{\n  \"DBSubnetGroupName\": \"demodbsubnetgroup\",\n  \"VpcId\": \"vpc-0cf40a5f6db5eb3cd\",\n  \"Subnets\": \"subnet-0c01a5ba480b930f4\"\n}\n{\n  \"DBSubnetGroupName\": \"demodbsubnetgroup\",\n  \"VpcId\": \"vpc-0cf40a5f6db5eb3cd\",\n  \"Subnets\": \"subnet-042a4bee8e92287e8\"\n}\n```\n\n### 2.4 Create a VPC Security Group \u003ca id=\"rds-create-vpc-security-group\"\u003e\u003c/a\u003e\n\nThe _penultimate_ step to creating the DB instance is creating a VPC security group, an instance level virtual firewall with _rules_ to control inbound and outbound traffic.\n\n```\n$ aws ec2 create-security-group --group-name DemoRDSSecurityGroup --description \"Demo RDS security group\" --vpc-id ${RDS_VPC_ID}\n{\n    \"GroupId\": \"sg-06800acf8d6279971\"\n}\n\n# Export the RDS VPC Security Group ID for easy reference in the subsequent commands\n$ export RDS_VPC_SECURITY_GROUP_ID=sg-06800acf8d6279971\n```\n\nWe will use this security group at a later point, to set an `inbound` rule to allow all traffic from the EKS cluster to the RDS instance.\n\n### 2.5 Create a DB Instance in the VPC \u003ca id=\"rds-create-db-instance-in-the-vpc\"\u003e\u003c/a\u003e\n\n```sh\n$ aws rds create-db-instance \\\n  --db-name demordsmyqldb \\\n  --db-instance-identifier demordsmyqldbinstance \\\n  --allocated-storage 10 \\\n  --db-instance-class db.t2.micro \\\n  --engine mysql \\\n  --engine-version \"5.7.26\" \\\n  --master-username demoappuser \\\n  --master-user-password demoappuserpassword \\\n  --no-publicly-accessible \\\n  --vpc-security-group-ids ${RDS_VPC_SECURITY_GROUP_ID} \\\n  --db-subnet-group-name \"demodbsubnetgroup\" \\\n  --availability-zone ap-south-1b \\\n  --port 3306 | jq '{DBInstanceIdentifier:.DBInstance.DBInstanceIdentifier,Engine:.DBInstance.Engine,DBName:.DBInstance.DBName,VpcSecurityGroups:.DBInstance.VpcSecurityGroups,EngineVersion:.DBInstance.EngineVersion,PubliclyAccessible:.DBInstance.PubliclyAccessible}'\n\n# Respone:\n{\n  \"DBInstanceIdentifier\": \"demordsmyqldbinstance\",\n  \"Engine\": \"mysql\",\n  \"DBName\": \"demordsmyqldb\",\n  \"VpcSecurityGroups\": [\n    {\n      \"VpcSecurityGroupId\": \"sg-06800acf8d6279971\",\n      \"Status\": \"active\"\n    }\n  ],\n  \"EngineVersion\": \"5.7.26\",\n  \"PubliclyAccessible\": false\n}\n```\n\nWe can verify that the DB instance has been created in the UI as well:\n\n\u003cMDImage src=\"/0/2_5reunqhdinqcp3vlrbq4.png\" alt=\"RDS MySQL DB Instance Details\" /\u003e\n\n\n### 2.6 Amazon RDS setup diagram \u003ca id=\"rds-setup-diagram\"\u003e\u003c/a\u003e\n\n\u003cMDImage src=\"/0/3_feg2ujod4ja6vtudique.jpeg\" alt=\"AWS RDS Setup Diagram\" /\u003e\n\n\n## 3. Setup the EKS cluster \u003ca id=\"setup-the-eks-cluster\"\u003e\u003c/a\u003e\n\nSpinning up an EKS cluster on AWS is as simple as:\n\n```sh\n$ eksctl create cluster --name=demo-eks-cluster --nodes=2 --region=ap-south-1\n[ℹ]  using region ap-south-1\n[ℹ]  setting availability zones to [ap-south-1a ap-south-1c ap-south-1b]\n[ℹ]  subnets for ap-south-1a - public:192.168.0.0/19 private:192.168.96.0/19\n[ℹ]  subnets for ap-south-1c - public:192.168.32.0/19 private:192.168.128.0/19\n[ℹ]  subnets for ap-south-1b - public:192.168.64.0/19 private:192.168.160.0/19\n[ℹ]  nodegroup \"ng-ae09882f\" will use \"ami-09c3eb35bb3be46a4\" [AmazonLinux2/1.12]\n[ℹ]  creating EKS cluster \"demo-eks-cluster\" in \"ap-south-1\" region\n[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup\n[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-south-1 --name=demo-eks-cluster'\n[ℹ]  2 sequential tasks: { create cluster control plane \"demo-eks-cluster\", create nodegroup \"ng-ae09882f\" }\n[ℹ]  building cluster stack \"eksctl-demo-eks-cluster-cluster\"\n[ℹ]  deploying stack \"eksctl-demo-eks-cluster-cluster\"\n[ℹ]  building nodegroup stack \"eksctl-demo-eks-cluster-nodegroup-ng-ae09882f\"\n[ℹ]  --nodes-min=2 was set automatically for nodegroup ng-ae09882f\n[ℹ]  --nodes-max=2 was set automatically for nodegroup ng-ae09882f\n[ℹ]  deploying stack \"eksctl-demo-eks-cluster-nodegroup-ng-ae09882f\"\n[✔]  all EKS cluster resource for \"demo-eks-cluster\" had been created\n[✔]  saved kubeconfig as \"/Users/Bensooraj/.kube/config\"\n[ℹ]  adding role \"arn:aws:iam::account_number:role/eksctl-demo-eks-cluster-nodegroup-NodeInstanceRole-1631FNZJZTDSK\" to auth ConfigMap\n[ℹ]  nodegroup \"ng-ae09882f\" has 0 node(s)\n[ℹ]  waiting for at least 2 node(s) to become ready in \"ng-ae09882f\"\n[ℹ]  nodegroup \"ng-ae09882f\" has 2 node(s)\n[ℹ]  node \"ip-192-168-30-190.ap-south-1.compute.internal\" is ready\n[ℹ]  node \"ip-192-168-92-207.ap-south-1.compute.internal\" is ready\n[ℹ]  kubectl command should work with \"/Users/Bensooraj/.kube/config\", try 'kubectl get nodes'\n[✔]  EKS cluster \"demo-eks-cluster\" in \"ap-south-1\" region is ready\n\n```\n\nWe will create a kubernetes `Service` named `mysql-service` of type `ExternalName` aliasing the RDS endpoint `demordsmyqldbinstance.cimllxgykuy3.ap-south-1.rds.amazonaws.com`.\n\nRun `kubectl apply -f mysql-service.yaml` to create the service.\n\n```yaml\n# mysql-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mysql-service\n  name: mysql-service\nspec:\n  externalName: demordsmyqldbinstance.cimllxgykuy3.ap-south-1.rds.amazonaws.com\n  selector:\n    app: mysql-service\n  type: ExternalName\nstatus:\n  loadBalancer: {}\n```\n\nNow, clients running inside the pods within the cluster can connect to the RDS instance using `mysql-service`.\n\nLet's test the connect using a throwaway `busybox` pod:\n\n```sh\n$ kubectl run -i --tty --rm debug --image=busybox --restart=Never -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # nc mysql-service 3306\n^Cpunt!\n\n```\n\nIt is evident that the pod is unable to get through! Let's solve the problem now.\n\n## 4. Let's build the bridge! \u003ca id=\"lets-build-the-bridge\"\u003e\u003c/a\u003e\n\nWe are going to create a [VPC Peering Connection][5] to facilitate communication between the resources in the two VPCs. According to the documentation:\n\n\u003e A **VPC peering connection** is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\n\n### 4.1 Create and Accept a VPC Peering Connection \u003ca id=\"create-and-accept-vpc-peering-connections\"\u003e\u003c/a\u003e\n\nTo create a VPC peering connection, navigate to:\n\n1. VPC console: https://console.aws.amazon.com/vpc/\n2. Select `Peering Connections` and click on `Create Peering Connection`.\n3. Configure the details as follows (select the EKS VPC as the `Requester` and the RDS VPC as the `Accepter`):\n\n   \u003cMDImage src=\"/0/4_5a2ilp9pzgiih073mlvp.png\" alt=\"Configuration\" /\u003e\n4. Click on `Create Peering Connection`\n   \u003cMDImage src=\"/0/5_9yw3pc84bwo31nihvcvg.jpg\" alt=\"Confirmation page\" /\u003e\n5. Select the `Peering Connection` that we just created. Click on `Actions` =\u003e `Accept`. Again, in the confirmation dialog box, click on `Yes, Accept`.\n   \u003cMDImage src=\"/0/6_i3pgejmth4xmhb88fybn.jpg\" alt=\"Yes, Accept\" /\u003e\n\nDon't forget to export the VPC Peering Connection ID:\n\n```bash\n$ export VPC_PEERING_CONNECTION_ID=pcx-0cc408e65493fe197\n```\n\n### 4.2 Update the EKS cluster VPC's route table \u003ca id=\"update-eks-cluster-vpc-route-table\"\u003e\u003c/a\u003e\n\n```sh\n# Fetch the route table associated with the 3 public subnets of the VPC created by `eksctl`:\n$ aws ec2 describe-route-tables --filters Name=\"tag:aws:cloudformation:logical-id\",Values=\"PublicRouteTable\" | jq '.RouteTables[0].RouteTableId'\n\"rtb-06103bd0704b3a9ee\"\n\n# For easy reference\nexport EKS_ROUTE_TABLE_ID=rtb-06103bd0704b3a9ee\n\n# Add route: All traffic to (destination) the RDS VPC CIDR block is via the VPC Peering Connection (target)\n$ aws ec2 create-route --route-table-id ${EKS_ROUTE_TABLE_ID} --destination-cidr-block 10.0.0.0/24 --vpc-peering-connection-id ${VPC_PEERING_CONNECTION_ID}\n{\n    \"Return\": true\n}\n```\n\n### 4.3 Update the RDS VPC's route table \u003ca id=\"update-rds-vpc-route-table\"\u003e\u003c/a\u003e\n\n```sh\n# Add route: All traffic to (destination) the EKS cluster CIDR block is via the VPC Peering Connection (target)\n$ aws ec2 create-route --route-table-id ${RDS_ROUTE_TABLE_ID} --destination-cidr-block 192.168.0.0/16 --vpc-peering-connection-id ${VPC_PEERING_CONNECTION_ID}\n{\n    \"Return\": true\n}\n```\n\n### 4.4 Update the RDS instance's security group \u003ca id=\"update-rds-instance-security-group\"\u003e\u003c/a\u003e\n\nAllow all ingress traffic from the EKS cluster to the RDS instance on port `3306`:\n\n```sh\n$ aws ec2 authorize-security-group-ingress --group-id ${RDS_VPC_SECURITY_GROUP_ID} --protocol tcp --port 3306 --cidr 192.168.0.0/16\n```\n\n## 5. Test the connection \u003ca id=\"test-the-connection\"\u003e\u003c/a\u003e\n\n```sh\n$ kubectl run -i --tty --rm debug --image=busybox --restart=Never -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # nc mysql-service 3306\nN\n5.7.26-logR\u0026=lk`xTH???mj\t_5#K)\u003emysql_native_password\n```\n\nWe can see that `busybox` can now successfully talk to the RDS instance using the service `mysql-service`.\n\nThat said, this is what our final setup looks like:\n\u003cMDImage src=\"/0/7_1ba38e5zu8i36egibtvc.jpeg\" alt=\"Final setup\" /\u003e\n\n**Note**:\nThis setup allows all pods in the EKS cluster to access the RDS instance. Depending on your use case, this may or may not be ideal to your architecture. To implement more fine-grained access control, considering setting up a [`NetworkPolicy`][6] resource.\n\nUseful resources:\n\n1. [Visual Subnet Calculator][4]\n2. [jq - Command-line JSON processor][7]\n3. [AWS CLI Command Reference][8]\n4. [AWS VPC Peering][5]\n\n[1]: https://aws.amazon.com/rds/mysql/\n[2]: https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\n[3]: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html#USER_VPC.Subnets\n[4]: http://www.davidc.net/sites/default/subnets/subnets.html\n[5]: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n[6]: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n[7]: https://github.com/stedolan/jq\n[8]: https://docs.aws.amazon.com/cli/latest/index.html\n","slug":"0-accessing-amazon-rds-from-aws-eks"}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"vZF5O8Cp0wJbkUP_UfCYA","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>